# -*- coding: utf-8 -*-
"""Slinky2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xIHTt82SBceEHmHUhLsz2G91Mna6zLTD
"""

import torch
from torch import nn
from torch.utils import data
import torch.nn.functional as F
import scipy.io as scio
import numpy as np
from matplotlib import pyplot as plt
import torch.onnx
import os
import random
import time
import math
import wandb
import shutil
from sklearn.model_selection import train_test_split
import copy
# from torchstat import stat
import argparse
import configparser
from utils import *
from model import *

from priority_memory import FastPriorReplayBuffer

# 1. Start a new run
# wandb.setup(wandb.Settings(program="D:/MatlabWorks/DER/DER_Slinky_Basic/CaseStudy/HORIZONTAL_HANGING/NNTRAINING#ANGLESOFT/slinky_Resnet_0.py"))
wandb.init(project='Slinky')
# please use your own account here
# wandb.init(project='Slinky', entity='')

# 2. Save model inputs and hyperparameters
config = wandb.config


config.start_learning_rate = lr
#config.architecture = "Subnet_Resnet_like"
config.architecture = "Subnet_MLP"
config.dataset = 'resample'
config.data_augmentation = "noise"
config.epoch = num_epochs
config.batch_size = batch_size
config.neurons_per_layer = NeuronsPerLayer
# config.noise_amp = noise_amp
config.miniBatch_maxNum = miniBatch_maxNum
#config.basic_block_layer = 4
# config.loss = "max_loss"
config.loss = "mse_loss"
config.weight_decay = weight_decay_param
# config.shear_data_k = shear_k
config.description = "try forcing NN to learn the concept of shear with anchor point method"
#config.focal_loss_gamma = focal_loss_gamma
wandb.run.name = currentTime
# shutil.copy("./slinky_Resnet_0.py", os.path.join(wandb.run.dir, "slinky_Resnet_0.py"))
# wandb.save('slinky_Resnet_0.py')

argparser = argparse.ArgumentParser()
argparser.add_argument("--device", type=str, help="The computing device")
argparser.add_argument("--init_learning_rate", type=float, help="Initial learning rate")
argparser.add_argument("--num_epochs", type=int, help="Number of training epochs")
argparser.add_argument("--batch_size", type=int, help="Training batch size")
argparser.add_argument("--architecture", choices=["MlpPure"], help="Model architecture")
argparser.add_argument("--layer_width", type=int, help="Number of neurons per layer")
argparser.add_argument("--weight_decay", type=float, help="Weight decay of the optimizer (similar to l2-regularization)")
argparser.add_argument("--config_file", type=str, default=None)
args = argparser.parse_args()

if args.config_file:
    configparser = configparser.ConfigParser()
    configparser.read(args.config_file)


device = torch.device("cuda:1")


seed_torch()

# loading in data
# resample datasets
# dataFile = '../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Small_Damping.mat'
# dataFile_Augmentation = ['../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Small_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Freeend.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Dropping.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_5G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_10G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Shear.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Large_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Large_Damping.mat']

dataFile = '../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Small_Damping.mat'
dataFile_Augmentation = ['../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Small_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Freeend.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Dropping.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_5G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_10G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Shear.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Large_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Large_Damping.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Imbalanced_Stretching.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Stretching_Shear.mat']

dataFile_Test = '../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Freeend.mat'
data2 = scio.loadmat(dataFile)
data_Test = scio.loadmat(dataFile_Test)
print("Data loaded")
data_input = data2['NNInput_All_reshape']
data_output = data2['NNOutput_All_reshape']
data_input, data_output = data_input.astype('float32'), data_output.astype('float32')

for file_name in dataFile_Augmentation:
    data_augmentation = scio.loadmat(file_name)
    data_augmentation_input = data_augmentation['NNInput_All_reshape']
    data_augmentation_output = data_augmentation['NNOutput_All_reshape']
    data_augmentation_input, data_augmentation_output = data_augmentation_input.astype('float32'), data_augmentation_output.astype('float32')
    data_input = np.concatenate((data_input,data_augmentation_input),axis=1)
    data_output = np.concatenate((data_output,data_augmentation_output),axis=1)
print('Data augmentation loaded')

data_input_Test = data_Test['NNInput_All_reshape']
data_output_Test = data_Test['NNOutput_All_reshape']
data_input_Test, data_output_Test = data_input_Test.astype('float32'), data_output_Test.astype('float32')

num_samples = data_input.shape[1] - 2
data_input = data_input[:,-num_samples:-1]
data_output = data_output[:,-num_samples:-1]

# constructing features_train, labels_train
# features_train = data_input
# labels_train = data_output
# features_train = torch.from_numpy(features_train).to(device)
# labels_train = torch.from_numpy(labels_train).to(device)
features_train, features_validate, labels_train, labels_validate = train_test_split(np.transpose(data_input), np.transpose(data_output), test_size=0.33)

# augment dataset with shear
# shear_k = 200
# features_shear, labels_shear = make_shear_data(shear_k)
# features_train = np.concatenate((features_train,features_shear),axis=0)
# labels_train = np.concatenate((labels_train,labels_shear),axis=0)

features_train, features_validate, labels_train, labels_validate = torch.from_numpy(np.transpose(features_train)).to(device), torch.from_numpy(np.transpose(features_validate)).to(device), torch.from_numpy(np.transpose(labels_train)).to(device), torch.from_numpy(np.transpose(labels_validate)).to(device)
features_Test, labels_Test = torch.from_numpy(data_input_Test).to(device), torch.from_numpy(data_output_Test).to(device)

# features_train = data_input
# labels_train = data_output
# features_train = torch.from_numpy(features_train).to(device)
# labels_train = torch.from_numpy(labels_train).to(device)

# perform data transformation and add new dat into the dataset
# mirror with respect to the z axis
features_new = features_train.clone()
features_new = features_new[torch.tensor([1, 0, 2, 5, 4, 3]),:]
features_new[3:6,:] = -features_new[3:6,:]
labels_new = labels_train.clone()
features_train_all = torch.cat((features_train,features_new),1)
labels_train_all = torch.cat((labels_train,labels_new),1)

# mirror with respect to the x axis
features_new = features_train.clone()
features_new[2,:] = -features_new[2,:]
features_new[3:6,:] = -features_new[3:6,:]
labels_new = labels_train.clone()
features_train_all = torch.cat((features_train_all,features_new),1)
labels_train_all = torch.cat((labels_train_all,labels_new),1)

# rotate by pi
features_new = features_train.clone()
features_new = features_new[torch.tensor([1, 0, 2, 5, 4, 3]),:]
features_new[2,:] = -features_new[2,:]
labels_new = labels_train.clone()
features_train_all = torch.cat((features_train_all,features_new),1)
labels_train_all = torch.cat((labels_train_all,labels_new),1)

torch.set_printoptions(precision=16)

# features_train = torch.cat((features_train,features_train**2,features_train**3),0)

#features_train = torch.nn.functional.normalize(features_train,2,1)
#labels_max = torch.norm(labels_train[0,:],p=2)
#labels_train = torch.nn.functional.normalize(labels_train,2,1)
 
_, features_mu, features_std = features_normalize(features_train_all)
_, labels_mu, labels_std = features_normalize(labels_train_all)

# normalization with z-score
features_train = features_normalize_withknown(features_train, features_mu, features_std)
labels_train = features_normalize_withknown(labels_train, labels_mu, labels_std)
features_validate = features_normalize_withknown(features_validate, features_mu, features_std)
labels_validate = features_normalize_withknown(labels_validate, labels_mu, labels_std)
features_Test = features_normalize_withknown(features_Test, features_mu, features_std)
labels_Test = features_normalize_withknown(labels_Test, labels_mu, labels_std)

# normalization with min-max
# features_train -= features_train.min(1, keepdim=True)[0]
# features_train /= features_train.max(1, keepdim=True)[0]
# labels_train -= labels_train.min(1, keepdim=True)[0]
# labels_train /= labels_train.max(1, keepdim=True)[0]
# features_validate -= features_validate.min(1, keepdim=True)[0]
# features_validate /= features_validate.max(1, keepdim=True)[0]
# labels_validate -= labels_validate.min(1, keepdim=True)[0]
# labels_validate /= labels_validate.max(1, keepdim=True)[0]
# features_Test -= features_Test.min(1, keepdim=True)[0]
# features_Test /= features_Test.max(1, keepdim=True)[0]
# labels_Test -= labels_Test.min(1, keepdim=True)[0]
# labels_Test /= labels_Test.max(1, keepdim=True)[0]
print("Data normalized")

# adding 0 mean Gaussian noise into the data
features_train_orig = features_train.clone()
labels_train_orig = labels_train.clone()
for _ in range(0):
    features_new = features_train_orig.clone()
    noise_amp = 0.001
    features_new = features_new + noise_amp * torch.randn_like(features_train_orig)
    labels_new = labels_train_orig.clone()
    features_train = torch.cat((features_train,features_new),1)
    labels_train = torch.cat((labels_train,labels_new),1)

#print(features_max,labels_max)

# recording the mu and std values into matlab matrices
names = ['features_mu', 'features_std', 'labels_mu', 'labels_std']
values = []
values.append(features_mu)
values.append(features_std)
values.append(labels_mu)
values.append(labels_std)
# constructing the dict
mdic = dict(zip(names,values))
# using scipy.io
scio.savemat("mu_std_Resnet.mat", mdic)

#features_train = torch.from_numpy(features_train)
#labels_train = torch.from_numpy(labels_train)

features_train = features_train.permute(1, 0)
labels_train = labels_train.permute(1, 0)
features_validate = features_validate.permute(1, 0)
labels_validate = labels_validate.permute(1, 0)
features_Test = features_Test.permute(1, 0)
labels_Test = labels_Test.permute(1, 0)

plotData = 0
if plotData == 1:
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')

    ax.set_xlabel('l_{i-1}')
    ax.set_ylabel('l_{i}')
    ax.set_zlabel('theta_i')

    xs = features_train[:,0].cpu().detach().numpy()
    ys = features_train[:,1].cpu().detach().numpy()
    zs = features_train[:,2].cpu().detach().numpy()
    v = labels_train.cpu().detach().numpy()
    c = np.abs(v)

    cmhot = plt.get_cmap("hot")
    ax.scatter(xs, ys, zs, v, s=5, c=c, cmap=cmhot)
    plt.show()
    input("look at the figures")

# construct the input and output for NN training
num_data = features_train.size(0)
batch_size = int(features_train.size()[0]/10) #1000
#batch_size = int(2)
train_iter = make_dataloader((features_train, labels_train), batch_size)
validate_iter = make_dataloader((features_validate, labels_validate), int(features_validate.size()[0]/100))
Test_iter = make_dataloader((features_Test, labels_Test), int(features_Test.size()[0]/100))
print("Dataset constructed")

# defining the neural network
NeuronsPerLayer = 16

# net = MLP_Poly_Resnet(NeuronsPerLayer).to(device)
net = MLPPure(NeuronsPerLayer,5).to(device)
print(" number of parameters: ", sum(param.numel() for param in net.parameters()))
# net = MLP_Pure_with_batchnorm(NeuronsPerLayer).to(device)
net.train()
# net.apply(init_weights)

# train the neural network
lr, num_epochs = 1e-2, 10+1
last_lr = 1e-4
lr_gamma = torch.exp(torch.log(torch.tensor(last_lr / lr)) / num_epochs)
miniBatch_maxNum = 100
loss = nn.MSELoss()
weight_decay_param = 0.000
# loss = nn.Softmax(dim=1)
# loss = nn.L1Loss()
trainer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=weight_decay_param)
# trainer = torch.optim.SGD(net.parameters(),lr=lr)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(trainer,patience=5,verbose=True)
# scheduler = torch.optim.lr_scheduler.ExponentialLR(trainer,lr_gamma.to(device),verbose=True)
focal_loss_gamma = 3

# tensorboard writting
currentTime = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())

loss_his = np.zeros(num_epochs)

# 3. Log gradients and model parameters
wandb.watch(net)
min_his = 1e4

pr_buffer = FastPriorReplayBuffer(num_data, total_episode=num_epochs)
# pr_buffer = FastPriorReplayBuffer(int(num_data*0.5), total_episode=num_epochs)

for X, y in train_iter:
    #l = loss(net(X), y)
    for xx, yy in zip(X, y):
        pr_buffer.append(features=torch.cat([xx, yy]).cpu().detach().numpy())

buffer_batch_size = 1024*10

for epoch in range(num_epochs):
    for _ in range(int(num_data / buffer_batch_size)):
        indices, data, weights = pr_buffer.sample_with_weights(batch_size=buffer_batch_size)
        # print(data)
        # print(data.shape)
        nn_input = torch.from_numpy(data[:,:-1]).to(device)
        target = torch.from_numpy(data[:,-1]).to(device)
        # input(f"nn_input.shape: {nn_input.shape}")
        predicted = net(nn_input)
        # input(f"predicted.shape: {predicted.shape}")
        abs_err = torch.abs(predicted.ravel() - target)

        pr_buffer.set_weights(indices, abs_err.cpu().detach().numpy())

        l = torch.mean(torch.from_numpy(weights).to(device) * abs_err)
        # l = torch.max(torch.abs(net(X)-y))
        #l = torch.mean(torch.abs(net(X)-y)**focal_loss_gamma)
        trainer.zero_grad()
        l.backward()
        trainer.step()

    pr_buffer.update_after_episode()

# for epoch in range(num_epochs):
#     for X, y in train_iter:
#         # l = loss(net(X), y)
#         # l = torch.max(torch.abs(net(X)-y))
#         temp_out = torch.abs(net(X)-y)
#         temp_out_sorted, indices = torch.sort(temp_out,dim=0,descending=True)
#         l = torch.sum(temp_out_sorted[0:miniBatch_maxNum])
#         # l = torch.mean(torch.abs(net(X)-y)**focal_loss_gamma)
#         trainer.zero_grad()
#         l.backward()
#         trainer.step()
    
#     scheduler.step(l)

    # if epoch%1 == 0:
    #     # notice! : this is mse loss
    #     l = loss(net(features_train), labels_train)
    #     loss_his[epoch] = l
    #     print(f'epoch {epoch + 1}, mse loss {l}')
    #     mse_l = l.item()
    #     wandb.log({"mse_loss": mse_l},step=epoch)
        
    #     l = torch.max(torch.abs(net(features_train)-labels_train))
    #     max_l = l.item()
    #     print(f'epoch {epoch + 1}, max loss {l}')
    #     wandb.log({"max_loss": max_l},step=epoch)
    
    # recording training metrics
    if epoch%1 == 0:
        # notice! : this is mse loss
        net.eval()
        l = 0
        l_max = -float("inf")
        with torch.no_grad():
            for X, y in train_iter:
                y_hat = net(X)
                l += loss(y_hat, y) * X.shape[0] # can be numerically unstable
                l_max = max(torch.max(torch.abs(y_hat - y)), l_max)
        l /= labels_train.shape[0]
        loss_his[epoch] = l
        print(f'epoch {epoch + 1}, training mse loss {l}')
        mse_l = l.item()
        wandb.log({"mse_loss": mse_l},step=epoch)
        
        max_l = l_max.item()
        print(f'epoch {epoch + 1}, training max loss {max_l}')
        wandb.log({"max_loss": max_l},step=epoch)
        if max_l < min_his:
            min_his = max_l
            # save_best_model()
            best_model = copy.deepcopy(net)
        net.train()
    scheduler.step(l)
    # scheduler.step()

    # recording validation metrics
    if epoch%1 == 0:
        # notice! : this is mse loss
        net.eval()
        l = 0
        l_max = -float("inf")
        with torch.no_grad():
            for X, y in validate_iter:
                y_hat = net(X)
                l += loss(y_hat, y) * X.shape[0] # can be numerically unstable
                l_max = max(torch.max(torch.abs(y_hat - y)), l_max)
        l /= labels_validate.shape[0]
        loss_his[epoch] = l
        print(f'epoch {epoch + 1}, validation mse loss {l}')
        mse_l = l.item()
        wandb.log({"validate_mse_loss": mse_l},step=epoch)
        
        max_l = l_max.item()
        print(f'epoch {epoch + 1}, validation max loss {max_l}')
        wandb.log({"validate_max_loss": max_l},step=epoch)
        net.train()

    # recording test metrics
    if epoch%100 == 0:
        # notice! : this is mse loss
        net.eval()
        l = 0
        l_max = -float("inf")
        with torch.no_grad():
            for X, y in Test_iter:
                y_hat = net(X)
                l += loss(y_hat, y) * X.shape[0] # can be numerically unstable
                l_max = max(torch.max(torch.abs(y_hat - y)), l_max)
        l /= labels_Test.shape[0]
        loss_his[epoch] = l
        print(f'epoch {epoch + 1}, Test mse loss {l}')
        mse_l = l.item()
        wandb.log({"Test_mse_loss": mse_l},step=epoch)
        
        max_l = l_max.item()
        print(f'epoch {epoch + 1}, Test max loss {max_l}')
        wandb.log({"Test_max_loss": max_l},step=epoch)
        net.train()

    if False and epoch%10 == 0:
        example = torch.rand(1,6).to("cpu")
        traced_script_module = torch.jit.trace(net.to("cpu"), example)
        traced_script_module = traced_script_module.to("cpu")
        traced_script_module.save("./"+currentTime+"/traced_slinky_resnet_current.pt")
        traced_script_module.save("./traced_slinky_resnet_current.pt")
        shutil.copy("./traced_slinky_resnet_current.pt", os.path.join(wandb.run.dir, "traced_slinky_resnet_current.pt"))
        wandb.save("traced_slinky_resnet_current.pt")
        net.to(device)
        #print("batchnorm1d.running_mean: ", net.layer2[0].running_mean)
    
    
# saving the neural network parameters into matlab .mat file
net.eval()

# names = ['W1','b1','W2','b2','W3','b3','W4','b4','W5','b5','W6','b6','W7','b7','W8','b8','W9','b9','W10','b10','W11','b11','W12','b12','W13','b13','W14','b14','W15','b15','W16','b16','W17','b17','W18','b18','W19','b19','W20','b20','W21','b21','W22','b22','W23','b23','W24','b24','W25','b25','W26','b26','W27','b27','W28','b28','W29','b29','W30','b30']
# values = []
# # print(net[0].parameters().numpy()) 
# for params in net.parameters():
#   # print(type(params.detach().numpy()))
#   # print(params)
#   values.append(params.cpu().detach().numpy())

# # constructing the dict
# mdic = dict(zip(names,values))
# # using scipy.io
# scio.savemat("matlab_matrix_Resnet.mat", mdic)

# data recording using ONNX
# batch_size = 1
# x = torch.randn(batch_size,6).to(device)
# torch_out = net(x)

# torch.onnx.export(net,
#                   x,
#                   'slinky_Resnet.onnx',
#                   export_params=True,
#                   opset_version=10,
#                   do_constant_folding=True,
#                   input_names=['input'],
#                   output_names=['output'],
#                   dynamic_axes={'input':{0:'batch_size'},
#                                 'output':{0:'batch_size'}})

# recording the data in the BatchNorm layer
#batch_mean = net.layer2[0].running_mean
#batch_var = net.layer2[0].running_var
#names = ['batch_mean','batch_var']
#values = []
#values.append(batch_mean.cpu().detach().numpy())
#values.append(batch_var.cpu().detach().numpy())

# constructing the dict
mdic = dict(zip(names,values))
# using scipy.io
scio.savemat("batch_Resnet.mat", mdic)
print(net(torch.from_numpy(np.array([0.1, 0.1, 0, 0.001, 0.001, 0.001])).float().reshape(1,6).to(device)))

# constructing the libtorch model
#example = torch.rand(1,3).to(device)
#traced_script_module = torch.jit.trace(net, example).to("cpu")
#traced_script_module.save("traced_slinky_resnet.pt")

"""# Visualize residuals"""
#net([0.1,0.1,0])
# predicted_energy = net(features_train)

# plt.plot(labels_train.cpu().detach().numpy(), predicted_energy.cpu().detach().numpy())

# fig, ax = plt.subplots()
# color = 'tab:red'
# ax.plot(np.divide(labels_train.cpu().detach().numpy() - predicted_energy.cpu().detach().numpy(), labels_train.cpu().detach().numpy()),color=color)
# ax.set_xlabel('sample num',fontsize=12)
# ax.set_ylabel('relative error',fontsize=12)
# ax.set_title('error plot',fontsize=12)
# ax.tick_params(axis='y',labelcolor=color)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)

# ax2 = ax.twinx()
# color = 'tab:blue'
# ax2.set_ylabel('absolute error',color=color,fontsize=12)
# ax2.plot(labels_train.cpu().detach().numpy() - predicted_energy.cpu().detach().numpy(), color=color)
# ax2.tick_params(axis='y',labelcolor=color)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)
# fig.tight_layout()

# fig, ax = plt.subplots()
# ax.plot(np.arange(num_epochs),np.log10(loss_his),label='training loss')
# ax.set_xlabel('epoch',fontsize=12)
# ax.set_ylabel('log10(training loss)',fontsize=12)
# ax.set_title('training loss history',fontsize=12)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)

# fig, ax = plt.subplots()
# ax.plot(labels_train.cpu().detach().numpy())
# ax.set_xlabel('sample num',fontsize=12)
# ax.set_ylabel('label value',fontsize=12)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)

# torch.mean((labels_train-predicted_energy)**2)

# fig = plt.figure()
# ax = fig.add_subplot(projection='3d')

# ax.set_xlabel('l_{i-1}')
# ax.set_ylabel('l_{i}')
# ax.set_zlabel('theta_i')

# xs = features_train[:,0].cpu().detach().numpy()
# ys = features_train[:,1].cpu().detach().numpy()
# zs = features_train[:,2].cpu().detach().numpy()
# v = labels_train.cpu().detach().numpy()
# c = np.abs(v)

# cmhot = plt.get_cmap("hot")
# ax.scatter(xs, ys, zs, v, s=5, c=c, cmap=cmhot)
# plt.show()


# fig = plt.figure()
# ax = fig.add_subplot(projection='3d')

# ax.set_xlabel('l_{i-1}')
# ax.set_ylabel('l_{i}')
# ax.set_zlabel('Energy')

# xs = features_train[:,0].cpu().detach().numpy()
# ys = features_train[:,1].cpu().detach().numpy()
# zs = labels_train[:,0].cpu().detach().numpy()

# ax.scatter(xs, ys, zs, 'tab:blue')

# ax.set_xlabel('l_{i-1}')
# ax.set_ylabel('l_{i}')
# ax.set_zlabel('Energy')

# xs = features_train[:,0].cpu().detach().numpy()
# ys = features_train[:,1].cpu().detach().numpy()
# zs = predicted_energy[:,0].cpu().detach().numpy()

# ax.scatter(xs, ys, zs, 'tab:red')
# plt.show()

example = torch.rand(1,6).to("cpu")
traced_script_module = torch.jit.trace(net.to("cpu"), example)
traced_script_module = traced_script_module.to("cpu")
traced_script_module.save("./"+currentTime+"/traced_slinky_resnet_final.pt")
traced_script_module.save("./traced_slinky_resnet_final.pt")
shutil.copy("./traced_slinky_resnet_final.pt", os.path.join(wandb.run.dir, "traced_slinky_resnet_final.pt"))
wandb.save("traced_slinky_resnet_final.pt")
net.to(device)

example = torch.rand(1,6).to("cpu")
traced_script_module = torch.jit.trace(best_model.to("cpu"), example)
traced_script_module = traced_script_module.to("cpu")
traced_script_module.save("./"+currentTime+"/traced_slinky_resnet_best.pt")
traced_script_module.save("./traced_slinky_resnet_best.pt")
shutil.copy("./traced_slinky_resnet_best.pt", os.path.join(wandb.run.dir, "traced_slinky_resnet_best.pt"))
wandb.save("traced_slinky_resnet_best.pt")
best_model.to(device)

wandb.finish()
