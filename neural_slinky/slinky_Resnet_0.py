# -*- coding: utf-8 -*-
"""Slinky2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xIHTt82SBceEHmHUhLsz2G91Mna6zLTD
"""

import torch
from torch import nn
from torch.utils import data
import torch.nn.functional as F
import scipy.io as scio
import numpy as np
from matplotlib import pyplot as plt
from IPython import display
import torch.onnx
from torchsampler import ImbalancedDatasetSampler
import os
import random
import time
import math
from torch.utils.tensorboard import SummaryWriter
import wandb
import shutil
from tqdm.auto import tqdm
from sklearn.model_selection import train_test_split
import copy
# from torchstat import stat

from priority_memory import FastPriorReplayBuffer

# 1. Start a new run
# wandb.setup(wandb.Settings(program="D:/MatlabWorks/DER/DER_Slinky_Basic/CaseStudy/HORIZONTAL_HANGING/NNTRAINING#ANGLESOFT/slinky_Resnet_0.py"))
wandb.init(project='Slinky', entity='liqiaofeng1990')
# please use your own account here
# wandb.init(project='Slinky', entity='')

# 2. Save model inputs and hyperparameters
config = wandb.config

plt.close('all')
device = torch.device("cuda:1")

# defining functions
def load_array(data_arrays, batch_size, is_train=True):
    """construct a PyTorch data iterator. """
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)#, num_workers=6)#, pin_memory=True)

def init_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.normal_(m.weight, std=0.1)

def features_normalize(data):
    mu = np.mean(data.cpu().numpy(),axis=1)
    std = np.std(data.cpu().numpy(),axis=1)
    # print(std.shape)
    # print(std.reshape(3,1).shape)
    # print(std)
    # print(std.reshape(3,1))
    out = (data.cpu().numpy() - mu.reshape(len(mu),1))/ (std.reshape(len(std),1))
    print(mu)
    print(std)
    return torch.from_numpy(out).to(device), mu, std

def features_normalize_withknown(data, mu, std):
    out = (data.cpu().numpy() - mu.reshape(len(mu),1))/ (std.reshape(len(std),1))
    return torch.from_numpy(out).to(device)

def chiral_transformation_x(data):
    # data is a 6-dim input vector
    # the output is a 6-dim vector, as the mirror of input data with respect to the x-axis
    #new_data = torch.zeros_like(data)
    new_data = data.clone()
    new_data[:,:2] = data[:,:2]
    new_data[:,2] = -data[:,2]
    new_data[:,3:] = -data[:,3:]
    return new_data

def chiral_transformation_z(data):
    # data is a 6-dim input vector
    # the output is a 6-dim vector, as the mirror of input data with respect to the z-axis
    #new_data = torch.zeros_like(data)
    new_data = data.clone()
    new_data[:,0] = data[:,1]
    new_data[:,1] = data[:,0]
    new_data[:,2] = data[:,2]
    new_data[:,3] = -data[:,5]
    new_data[:,4] = -data[:,4]
    new_data[:,5] = -data[:,3]
    return new_data

def chiral_transformation_xz(data):
    # data is a 6-dim input vector
    # the output is a 6-dim vector, as the 180 degrees rotation of input data
    #new_data = torch.zeros_like(data)
    new_data = data.clone()
    new_data = chiral_transformation_x(data)
    new_data = chiral_transformation_z(new_data)
    return new_data

def make_tan(data):
    # data is a 6-dim input vector
    # the output is a 6-dim vector, as the 180 degrees rotation of input data
    #new_data = torch.zeros_like(data)
    new_data = data.clone()
    new_data[:,0] = data[:,0]
    new_data[:,1] = data[:,1]
    new_data[:,2] = data[:,2]
    new_data[:,3] = torch.tan(new_data[:,3])
    new_data[:,4] = torch.tan(new_data[:,4])
    new_data[:,5] = torch.tan(new_data[:,5])
    return new_data

def region_shifting(data,data_mean,data_std):
    new_data = data.clone()
    new_data[:,0] = data[:,0]
    new_data[:,1] = data[:,1]
    new_data[:,2] = data[:,2]
    new_data[:,3] = data[:,3]
    new_data[:,5] = data[:,5]
    # int_3 = torch.ceil(-(data_mean[3] + data_std[3] * data[:,3]) / math.pi)
    int_4 = torch.ceil(-(data_mean[4] + data_std[4] * data[:,4]) / math.pi)
    # int_5 = torch.ceil(-(data_mean[5] + data_std[5] * data[:,5]) / math.pi)
    # new_data[:,3] = data[:,3] + int_3 * math.pi / data_std[3] + data_mean[3] / data_std[3]
    new_data[:,4] = data[:,4] + int_4 * math.pi / data_std[4] + data_mean[4] / data_std[4]
    # new_data[:,5] = data[:,5] + int_5 * math.pi / data_std[5] + data_mean[5] / data_std[5]
    # print(sum(new_data[:,3] >= math.pi))
    # print(sum(new_data[:,4] >= math.pi))
    # print(sum(new_data[:,5] >= math.pi))
    # print(sum(new_data[:,3] < 0))
    # print(sum(new_data[:,4] < 0))
    # print(sum(new_data[:,5] < 0))
    # assert(sum(new_data[:,3] >= math.pi) == 0)
    # assert(sum(new_data[:,4] >= math.pi) == 0)
    # assert(sum(new_data[:,5] >= math.pi) == 0)
    # assert(sum(new_data[:,3] < 0) == 0)
    # assert(sum(new_data[:,4] < 0) == 0)
    # assert(sum(new_data[:,5] < 0) == 0)
    return new_data 

def region_shifting2(data,data_mean,data_std):
    new_data = data.clone()
    new_data[:,0] = data[:,0]
    new_data[:,1] = data[:,1]
    new_data[:,2] = data[:,2]
    new_data[:,3] = data[:,3]
    new_data[:,5] = data[:,5]
    # int_3 = torch.ceil(-0.5-(data_mean[3] + data_std[3] * data[:,3]) / math.pi)
    int_4 = torch.ceil(-0.5-(data_mean[4] + data_std[4] * data[:,4]) / math.pi)
    # int_5 = torch.ceil(-0.5-(data_mean[5] + data_std[5] * data[:,5]) / math.pi)
    # new_data[:,3] = data[:,3] + int_3 * math.pi / data_std[3] + data_mean[3] / data_std[3]
    new_data[:,4] = data[:,4] + int_4 * math.pi / data_std[4] + data_mean[4] / data_std[4]
    # new_data[:,5] = data[:,5] + int_5 * math.pi / data_std[5] + data_mean[5] / data_std[5]
    return new_data

# def finding_region(data):
#     return 

class RotationInvariantLayer(nn.Module):
    def __init__(self,NeuronsPerLayer=32):
        #super(RotationInvariantLayer, self).__init__()
        #self.sigma = 0.01.to(device)
        super().__init__()
        self.w1 = nn.Parameter(torch.randn(NeuronsPerLayer,3))
        self.w2 = nn.Parameter(torch.randn(NeuronsPerLayer,1))
        self.w3 = nn.Parameter(torch.randn(NeuronsPerLayer,1))
        self.bias = nn.Parameter(torch.randn(NeuronsPerLayer))
        #self.weight = torch.cat([self.w1,self.w2,self.w3,-self.w2-self.w3],1)
        
    def get_weight_(self):
        weight = torch.cat([self.w1,self.w2,self.w3,-self.w2-self.w3],1)
        bias = self.bias
        return weight, bias

    def forward(self,x):
        #formal_weight, formal_bias = self.get_weight_()
        weight = torch.cat([self.w1,self.w2,self.w3,-self.w2-self.w3],1)
        #return F.linear(x,self.weight,self.bias).to(device)
        out = F.linear(x,weight,self.bias)
        #out = x * torch.transpose(weight,0,1) + self.bias
        return out

class Basic_Block(nn.Module):
    def __init__(self,NeuronsPerLayer):
        super().__init__()
        self.layer1 = nn.Linear(NeuronsPerLayer,NeuronsPerLayer)
        self.layer2 = nn.Linear(NeuronsPerLayer,NeuronsPerLayer)
        self.relu = nn.ReLU()
        self.soft = nn.Softplus()
    
    def forward(self,x):
        identity = x

        out = self.layer1(x)
        out = self.soft(out)
        out = self.layer2(out)
        out = self.soft(out)

        out2 = out + identity
        return out2

class DenseBlock(nn.Module):
    def __init__(self, NeuronsPerLayer, NumLayer):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(NumLayer):
            layer.append(
                nn.Sequential(
                # nn.BatchNorm1d(NeuronsPerLayer * i + NeuronsPerLayer),
                nn.Linear(NeuronsPerLayer * i + NeuronsPerLayer, NeuronsPerLayer), 
                nn.Softplus()#, Square()
                )
                )
                # nn.Linear(NeuronsPerLayer * i + NeuronsPerLayer, NeuronsPerLayer)), nn.Tanh()
        # layer.append(
        #     nn.Sequential(
        #     # nn.BatchNorm1d(NeuronsPerLayer * i + NeuronsPerLayer),
        #     nn.Linear(NeuronsPerLayer * (NumLayer-1) + NeuronsPerLayer, NeuronsPerLayer), 
        #     nn.Softplus(), Square()
        #     )
        #     )
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # Concatenate the input and output of each block on the channel
            # dimension
            X = torch.cat((X, Y), dim=-1)
            # X = torch.cat((X, Y), dim=1)
        # print("inside dense block")
        # print(X.size())
        return X

class MLP_Poly_Resnet(nn.Module):
    def __init__(self, NeuronsPerLayer=32):
        super(MLP_Poly_Resnet, self).__init__()
        self.layer1 = nn.Sequential(
            #nn.Linear(6,NeuronsPerLayer)
            RotationInvariantLayer(NeuronsPerLayer)
        )
        self.layer2 = nn.Sequential(
            #nn.BatchNorm1d(3*NeuronsPerLayer),
            nn.Linear(3*NeuronsPerLayer,NeuronsPerLayer),
            nn.Softplus()
        )
        self.layer3 = nn.Sequential(
            #Basic_Block(NeuronsPerLayer),
            #Basic_Block(NeuronsPerLayer),
            #Basic_Block(NeuronsPerLayer),
            Basic_Block(NeuronsPerLayer),
            Basic_Block(NeuronsPerLayer),
            Basic_Block(NeuronsPerLayer),
            Basic_Block(NeuronsPerLayer)
        )
        self.layer4 = nn.Sequential(
            nn.Linear(NeuronsPerLayer,1)
        )
        
    def forward(self, x):
        # with torch.no_grad():
        # augmented_x = torch.stack([x.cpu(), chiral_transformation_x(x.cpu()), chiral_transformation_z(x.cpu()), chiral_transformation_xz(x.cpu())]).to(device)
        # augmented_x = torch.stack([x, chiral_transformation_x(x), chiral_transformation_z(x), chiral_transformation_xz(x)])
        #print(augmented_x.shape)
        out = self.layer1(augmented_x)
        out = torch.cat([out, out**2, out**3], dim=-1)
        #print(out.shape)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = torch.sum(out, dim=0, keepdim=False)
        return out

class Softplus2(nn.Module):
    # def __init__(self):
        # super().__init__()
        # self.slope = slope
        # self.threshold = threshold

    def forward(self, x):
        # print(x.size())
        # temp = torch.zeros_like(x)

        # temp = torch.abs(x) * x
        # out = torch.zeros_like(x)
        # mask = x.ge(4.4721)
        # out[mask] = torch.square(x[mask])
        # out[~mask] = torch.log(1+torch.exp(torch.abs(x[~mask]) * x[~mask]))
        # return out

        # for ii in range(len(x)):
        #     if torch.abs(x[ii]) * x[ii] > self.threshold:
        #         temp[ii] = x[ii] ** 2
        #     else:
        #         temp[ii] = torch.log(1+torch.exp(self.slope * torch.abs(x[ii]) * x[ii]))/self.slope
        # if torch.abs(x) * x > self.threshold:
        #     return x**2
        # else:
        # return torch.log(1+torch.exp(self.slope * torch.abs(x) * x))/self.slope
        # return (x>4.4721) * torch.abs(x) * x + (x<=4.4721) * torch.log(1+torch.exp(torch.abs(x) * x))
        return (x>4.4721) * torch.abs(x) * x + (x<=4.4721) * torch.log(1+torch.exp(torch.abs(torch.clamp(x,max=4.4721)) * torch.clamp(x,max=4.4721)))
        # return torch.square(nn.Softplus(x))

class Square(nn.Module):
    def forward(self,x):
        return torch.square(x)

class MLP_Pure(nn.Module):
    def __init__(self, NeuronsPerLayer=32, NumLayer=4):
        super(MLP_Pure, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Linear(6,NeuronsPerLayer),
            #RotationInvariantLayer(NeuronsPerLayer),
            nn.Softplus()
        )
        self.layer2 = nn.Sequential(
            DenseBlock(NeuronsPerLayer,NumLayer)
        )
        self.layer3 = nn.Sequential(
            # nn.Linear(int(NeuronsPerLayer/8),1)
            nn.Linear(int(NeuronsPerLayer*(NumLayer+1)),1)
        )
        
    def forward(self, y):

        # augmented_x = torch.stack([x, chiral_transformation_x(x), chiral_transformation_z(x), chiral_transformation_xz(x)], dim=0)
        # augmented_x = torch.stack([make_tan(x), make_tan(chiral_transformation_x(x)), make_tan(chiral_transformation_z(x)), make_tan(chiral_transformation_xz(x))], dim=0)

        x = region_shifting2(y,features_mu,features_std)
        augmented_x = torch.stack([x, chiral_transformation_x(x), chiral_transformation_z(x), chiral_transformation_xz(x)], dim=0)
        out = self.layer1(augmented_x)
        out = self.layer2(out)
        out = self.layer3(out)
        out = torch.sum(out, dim=0, keepdim=False)
        return out

        # out = self.layer1(x)
        # out = self.layer2(out)
        # out = self.layer3(out)
        # return out

        # x1 = x
        # x2 = chiral_transformation_x(x)
        # x3 = chiral_transformation_z(x)
        # x4 = chiral_transformation_xz(x)

        # out1 = self.layer1(x1)
        # out1 = self.layer2(out1)
        # out1 = self.layer3(out1)
        # result = out1

        # out2 = self.layer1(x2)
        # out2 = self.layer2(out2)
        # out2 = self.layer3(out2)
        # result += out2

        # out3 = self.layer1(x3)
        # out3 = self.layer2(out3)
        # out3 = self.layer3(out3)
        # result += out3

        # out4 = self.layer1(x4)
        # out4 = self.layer2(out4)
        # out4 = self.layer3(out4)
        # result += out4

        # return result

        # y = region_shifting(x,features_mu,features_std)
        # augmented_x = torch.stack([y, chiral_transformation_x(y), chiral_transformation_z(y), chiral_transformation_xz(y)], dim=0)
        # out = self.layer1(augmented_x)
        # # out = self.layer1(x)
        # # out = torch.cat([out, out**2, out**3], dim=-1)
        # out = self.layer2(out)
        # out = self.layer3(out)
        # out = torch.sum(out, dim=0, keepdim=False)

        # y2 = region_shifting2(x,features_mu,features_std)
        # augmented_x2 = torch.stack([y2, chiral_transformation_x(y2), chiral_transformation_z(y2), chiral_transformation_xz(y2)], dim=0)
        # out2 = self.layer1(augmented_x2)
        # # out = self.layer1(x)
        # # out = torch.cat([out, out**2, out**3], dim=-1)
        # out2 = self.layer2(out2)
        # out2 = self.layer3(out2)
        # out2 = torch.sum(out2, dim=0, keepdim=False)
        # return 0.5*(out + out2)
        # return out2

class MLP_Pure_with_batchnorm(nn.Module):
    def __init__(self, NeuronsPerLayer=32):
        super(MLP_Pure_with_batchnorm, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Linear(6,NeuronsPerLayer), nn.Softplus()
        )
        self.layer2 = nn.Sequential(
            nn.BatchNorm1d(NeuronsPerLayer),
            nn.Linear(NeuronsPerLayer,NeuronsPerLayer), nn.Softplus(),
            # nn.BatchNorm1d(NeuronsPerLayer),
            nn.Linear(NeuronsPerLayer,NeuronsPerLayer), nn.Softplus(),
            # nn.BatchNorm1d(NeuronsPerLayer),
            nn.Linear(NeuronsPerLayer,NeuronsPerLayer), nn.Softplus()
        )
        self.layer3 = nn.Sequential(
            nn.Linear(int(NeuronsPerLayer),1)
        )
        
    def forward(self, x):
        x1 = x
        x2 = chiral_transformation_x(x)
        x3 = chiral_transformation_z(x)
        x4 = chiral_transformation_xz(x)

        out1 = self.layer1(x1)
        out1 = self.layer2(out1)
        out1 = self.layer3(out1)

        out2 = self.layer1(x2)
        out2 = self.layer2(out2)
        out2 = self.layer3(out2)

        out3 = self.layer1(x3)
        out3 = self.layer2(out3)
        out3 = self.layer3(out3)

        out4 = self.layer1(x4)
        out4 = self.layer2(out4)
        out4 = self.layer3(out4)
        return out1 + out2 + out3 + out4

def make_shear_data(shear_k = 1000):
    # a range of l1 and l2
    l1 = np.random.uniform(0.02,0.04,1000)
    l2 = np.random.uniform(0.02,0.04,1000)
    # theta will remain 0
    theta = np.zeros_like(l1)
    # gamma_1 and gamma_3 will remain 0
    gamma_1 = np.zeros_like(theta)
    gamma_3 = gamma_1
    # a range of gamma_2
    gamma_2 = np.random.uniform(math.pi/4,math.pi/2,len(gamma_1))

    # the stiffness k for shear
    # shear_k = 100;

    # making augment dataset
    shear_data_input = np.stack((l1,l2,theta,gamma_1,gamma_2,gamma_3),1)
    # making output labels
    shear_data_output = np.zeros_like(gamma_2)
    for ii in range(len(gamma_1)):
        shear_data_output[ii] = 0.5 * shear_k * ((l1[ii] * math.cos(gamma_2[ii] + math.pi/2)) **2 + (l2[ii] * math.cos(gamma_2[ii] + math.pi/2)) **2)

    return shear_data_input.astype('float32'), shear_data_output.reshape(-1,1).astype('float32')

def save_best_model():
    example = torch.rand(1,6)
    traced_script_module = torch.jit.trace(net, example.to(device))
    traced_script_module = traced_script_module.to("cpu")
    traced_script_module.save("./"+currentTime+"/traced_slinky_resnet.pt")
    traced_script_module.save("./traced_slinky_resnet.pt")
    shutil.copy("./traced_slinky_resnet.pt", os.path.join(wandb.run.dir, "traced_slinky_resnet.pt"))
    wandb.save("traced_slinky_resnet.pt")
    net.to(device)

def seed_torch(seed=1029):
	random.seed(seed)
	os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现
	np.random.seed(seed)
	torch.manual_seed(seed)
	torch.cuda.manual_seed(seed)
	torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
	torch.backends.cudnn.benchmark = False
	torch.backends.cudnn.deterministic = True

seed_torch()

# loading in data
# resample datasets
# dataFile = '../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Small_Damping.mat'
# dataFile_Augmentation = ['../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Small_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Freeend.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Dropping.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_5G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_10G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Shear.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Large_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Resample_Large_Damping.mat']

dataFile = '../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Small_Damping.mat'
dataFile_Augmentation = ['../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Small_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Freeend.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Dropping.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_5G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_10G.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Shear.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Large_Gravity.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Large_Damping.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Imbalanced_Stretching.mat','../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Stretching_Shear.mat']

dataFile_Test = '../RAWDATA#ANGLENEW#CHANGE#RELATIVE/SlinkyData_Angle_6_new_Freeend.mat'
data2 = scio.loadmat(dataFile)
data_Test = scio.loadmat(dataFile_Test)
print("Data loaded")
data_input = data2['NNInput_All_reshape']
data_output = data2['NNOutput_All_reshape']
data_input, data_output = data_input.astype('float32'), data_output.astype('float32')

for file_name in dataFile_Augmentation:
    data_augmentation = scio.loadmat(file_name)
    data_augmentation_input = data_augmentation['NNInput_All_reshape']
    data_augmentation_output = data_augmentation['NNOutput_All_reshape']
    data_augmentation_input, data_augmentation_output = data_augmentation_input.astype('float32'), data_augmentation_output.astype('float32')
    data_input = np.concatenate((data_input,data_augmentation_input),axis=1)
    data_output = np.concatenate((data_output,data_augmentation_output),axis=1)
print('Data augmentation loaded')

data_input_Test = data_Test['NNInput_All_reshape']
data_output_Test = data_Test['NNOutput_All_reshape']
data_input_Test, data_output_Test = data_input_Test.astype('float32'), data_output_Test.astype('float32')

num_samples = data_input.shape[1] - 2
data_input = data_input[:,-num_samples:-1]
data_output = data_output[:,-num_samples:-1]

# constructing features_train, labels_train
# features_train = data_input
# labels_train = data_output
# features_train = torch.from_numpy(features_train).to(device)
# labels_train = torch.from_numpy(labels_train).to(device)
features_train, features_validate, labels_train, labels_validate = train_test_split(np.transpose(data_input), np.transpose(data_output), test_size=0.33)

# augment dataset with shear
# shear_k = 200
# features_shear, labels_shear = make_shear_data(shear_k)
# features_train = np.concatenate((features_train,features_shear),axis=0)
# labels_train = np.concatenate((labels_train,labels_shear),axis=0)

features_train, features_validate, labels_train, labels_validate = torch.from_numpy(np.transpose(features_train)).to(device), torch.from_numpy(np.transpose(features_validate)).to(device), torch.from_numpy(np.transpose(labels_train)).to(device), torch.from_numpy(np.transpose(labels_validate)).to(device)
features_Test, labels_Test = torch.from_numpy(data_input_Test).to(device), torch.from_numpy(data_output_Test).to(device)

# features_train = data_input
# labels_train = data_output
# features_train = torch.from_numpy(features_train).to(device)
# labels_train = torch.from_numpy(labels_train).to(device)

# perform data transformation and add new dat into the dataset
# mirror with respect to the z axis
features_new = features_train.clone()
features_new = features_new[torch.tensor([1, 0, 2, 5, 4, 3]),:]
features_new[3:6,:] = -features_new[3:6,:]
labels_new = labels_train.clone()
features_train_all = torch.cat((features_train,features_new),1)
labels_train_all = torch.cat((labels_train,labels_new),1)

# mirror with respect to the x axis
features_new = features_train.clone()
features_new[2,:] = -features_new[2,:]
features_new[3:6,:] = -features_new[3:6,:]
labels_new = labels_train.clone()
features_train_all = torch.cat((features_train_all,features_new),1)
labels_train_all = torch.cat((labels_train_all,labels_new),1)

# rotate by pi
features_new = features_train.clone()
features_new = features_new[torch.tensor([1, 0, 2, 5, 4, 3]),:]
features_new[2,:] = -features_new[2,:]
labels_new = labels_train.clone()
features_train_all = torch.cat((features_train_all,features_new),1)
labels_train_all = torch.cat((labels_train_all,labels_new),1)

torch.set_printoptions(precision=16)

# features_train = torch.cat((features_train,features_train**2,features_train**3),0)

#features_train = torch.nn.functional.normalize(features_train,2,1)
#labels_max = torch.norm(labels_train[0,:],p=2)
#labels_train = torch.nn.functional.normalize(labels_train,2,1)
 
_, features_mu, features_std = features_normalize(features_train_all)
_, labels_mu, labels_std = features_normalize(labels_train_all)

# normalization with z-score
features_train = features_normalize_withknown(features_train, features_mu, features_std)
labels_train = features_normalize_withknown(labels_train, labels_mu, labels_std)
features_validate = features_normalize_withknown(features_validate, features_mu, features_std)
labels_validate = features_normalize_withknown(labels_validate, labels_mu, labels_std)
features_Test = features_normalize_withknown(features_Test, features_mu, features_std)
labels_Test = features_normalize_withknown(labels_Test, labels_mu, labels_std)

# normalization with min-max
# features_train -= features_train.min(1, keepdim=True)[0]
# features_train /= features_train.max(1, keepdim=True)[0]
# labels_train -= labels_train.min(1, keepdim=True)[0]
# labels_train /= labels_train.max(1, keepdim=True)[0]
# features_validate -= features_validate.min(1, keepdim=True)[0]
# features_validate /= features_validate.max(1, keepdim=True)[0]
# labels_validate -= labels_validate.min(1, keepdim=True)[0]
# labels_validate /= labels_validate.max(1, keepdim=True)[0]
# features_Test -= features_Test.min(1, keepdim=True)[0]
# features_Test /= features_Test.max(1, keepdim=True)[0]
# labels_Test -= labels_Test.min(1, keepdim=True)[0]
# labels_Test /= labels_Test.max(1, keepdim=True)[0]
print("Data normalized")

# adding 0 mean Gaussian noise into the data
features_train_orig = features_train.clone()
labels_train_orig = labels_train.clone()
for _ in range(0):
    features_new = features_train_orig.clone()
    noise_amp = 0.001
    features_new = features_new + noise_amp * torch.randn_like(features_train_orig)
    labels_new = labels_train_orig.clone()
    features_train = torch.cat((features_train,features_new),1)
    labels_train = torch.cat((labels_train,labels_new),1)

#print(features_max,labels_max)

# recording the mu and std values into matlab matrices
names = ['features_mu', 'features_std', 'labels_mu', 'labels_std']
values = []
values.append(features_mu)
values.append(features_std)
values.append(labels_mu)
values.append(labels_std)
# constructing the dict
mdic = dict(zip(names,values))
# using scipy.io
scio.savemat("mu_std_Resnet.mat", mdic)

#features_train = torch.from_numpy(features_train)
#labels_train = torch.from_numpy(labels_train)

features_train = features_train.permute(1, 0)
labels_train = labels_train.permute(1, 0)
features_validate = features_validate.permute(1, 0)
labels_validate = labels_validate.permute(1, 0)
features_Test = features_Test.permute(1, 0)
labels_Test = labels_Test.permute(1, 0)

plotData = 0
if plotData == 1:
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')

    ax.set_xlabel('l_{i-1}')
    ax.set_ylabel('l_{i}')
    ax.set_zlabel('theta_i')

    xs = features_train[:,0].cpu().detach().numpy()
    ys = features_train[:,1].cpu().detach().numpy()
    zs = features_train[:,2].cpu().detach().numpy()
    v = labels_train.cpu().detach().numpy()
    c = np.abs(v)

    cmhot = plt.get_cmap("hot")
    ax.scatter(xs, ys, zs, v, s=5, c=c, cmap=cmhot)
    plt.show()
    input("look at the figures")

# construct the input and output for NN training
num_data = features_train.size(0)
batch_size = int(features_train.size()[0]/10) #1000
#batch_size = int(2)
train_iter = load_array((features_train, labels_train), batch_size)
validate_iter = load_array((features_validate, labels_validate), int(features_validate.size()[0]/100))
Test_iter = load_array((features_Test, labels_Test), int(features_Test.size()[0]/100))
print("Dataset constructed")

# defining the neural network
NeuronsPerLayer = 16

# net = MLP_Poly_Resnet(NeuronsPerLayer).to(device)
net = MLP_Pure(NeuronsPerLayer,5).to(device)
print(" number of parameters: ", sum(param.numel() for param in net.parameters()))
# net = MLP_Pure_with_batchnorm(NeuronsPerLayer).to(device)
net.train()
# net.apply(init_weights)

# train the neural network
lr, num_epochs = 1e-2, 10+1
last_lr = 1e-4
lr_gamma = torch.exp(torch.log(torch.tensor(last_lr / lr)) / num_epochs)
miniBatch_maxNum = 100
loss = nn.MSELoss()
weight_decay_param = 0.000
# loss = nn.Softmax(dim=1)
# loss = nn.L1Loss()
trainer = torch.optim.Adam(net.parameters(),lr=lr,weight_decay=weight_decay_param)
# trainer = torch.optim.SGD(net.parameters(),lr=lr)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(trainer,patience=5,verbose=True)
# scheduler = torch.optim.lr_scheduler.ExponentialLR(trainer,lr_gamma.to(device),verbose=True)
focal_loss_gamma = 3

# tensorboard writting
currentTime = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
writer = SummaryWriter(currentTime)

loss_his = np.zeros(num_epochs)

config.start_learning_rate = lr
#config.architecture = "Subnet_Resnet_like"
config.architecture = "Subnet_MLP"
config.dataset = 'resample'
config.data_augmentation = "noise"
config.epoch = num_epochs
config.batch_size = batch_size
config.neurons_per_layer = NeuronsPerLayer
# config.noise_amp = noise_amp
config.miniBatch_maxNum = miniBatch_maxNum
#config.basic_block_layer = 4
# config.loss = "max_loss"
config.loss = "mse_loss"
config.weight_decay = weight_decay_param
# config.shear_data_k = shear_k
config.description = "try forcing NN to learn the concept of shear with anchor point method"
#config.focal_loss_gamma = focal_loss_gamma
wandb.run.name = currentTime
shutil.copy("./slinky_Resnet_0.py", os.path.join(wandb.run.dir, "slinky_Resnet_0.py"))
wandb.save('slinky_Resnet_0.py')

# 3. Log gradients and model parameters
wandb.watch(net)
min_his = 1e4

pr_buffer = FastPriorReplayBuffer(num_data, total_episode=num_epochs)
# pr_buffer = FastPriorReplayBuffer(int(num_data*0.5), total_episode=num_epochs)

for X, y in train_iter:
    #l = loss(net(X), y)
    for xx, yy in zip(X, y):
        pr_buffer.append(features=torch.cat([xx, yy]).cpu().detach().numpy())

buffer_batch_size = 1024*10

for epoch in range(num_epochs):
    for _ in range(int(num_data / buffer_batch_size)):
        indices, data, weights = pr_buffer.sample_with_weights(batch_size=buffer_batch_size)
        # print(data)
        # print(data.shape)
        nn_input = torch.from_numpy(data[:,:-1]).to(device)
        target = torch.from_numpy(data[:,-1]).to(device)
        # input(f"nn_input.shape: {nn_input.shape}")
        predicted = net(nn_input)
        # input(f"predicted.shape: {predicted.shape}")
        abs_err = torch.abs(predicted.ravel() - target)

        pr_buffer.set_weights(indices, abs_err.cpu().detach().numpy())

        l = torch.mean(torch.from_numpy(weights).to(device) * abs_err)
        # l = torch.max(torch.abs(net(X)-y))
        #l = torch.mean(torch.abs(net(X)-y)**focal_loss_gamma)
        trainer.zero_grad()
        l.backward()
        trainer.step()

    pr_buffer.update_after_episode()

# for epoch in range(num_epochs):
#     for X, y in train_iter:
#         # l = loss(net(X), y)
#         # l = torch.max(torch.abs(net(X)-y))
#         temp_out = torch.abs(net(X)-y)
#         temp_out_sorted, indices = torch.sort(temp_out,dim=0,descending=True)
#         l = torch.sum(temp_out_sorted[0:miniBatch_maxNum])
#         # l = torch.mean(torch.abs(net(X)-y)**focal_loss_gamma)
#         trainer.zero_grad()
#         l.backward()
#         trainer.step()
    
#     scheduler.step(l)

    # if epoch%1 == 0:
    #     # notice! : this is mse loss
    #     l = loss(net(features_train), labels_train)
    #     loss_his[epoch] = l
    #     print(f'epoch {epoch + 1}, mse loss {l}')
    #     mse_l = l.item()
    #     wandb.log({"mse_loss": mse_l},step=epoch)
        
    #     l = torch.max(torch.abs(net(features_train)-labels_train))
    #     max_l = l.item()
    #     print(f'epoch {epoch + 1}, max loss {l}')
    #     wandb.log({"max_loss": max_l},step=epoch)
    
    # recording training metrics
    if epoch%1 == 0:
        # notice! : this is mse loss
        net.eval()
        l = 0
        l_max = -float("inf")
        with torch.no_grad():
            for X, y in train_iter:
                y_hat = net(X)
                l += loss(y_hat, y) * X.shape[0] # can be numerically unstable
                l_max = max(torch.max(torch.abs(y_hat - y)), l_max)
        l /= labels_train.shape[0]
        loss_his[epoch] = l
        print(f'epoch {epoch + 1}, training mse loss {l}')
        mse_l = l.item()
        wandb.log({"mse_loss": mse_l},step=epoch)
        
        max_l = l_max.item()
        print(f'epoch {epoch + 1}, training max loss {max_l}')
        wandb.log({"max_loss": max_l},step=epoch)
        if max_l < min_his:
            min_his = max_l
            # save_best_model()
            best_model = copy.deepcopy(net)
        net.train()
    scheduler.step(l)
    # scheduler.step()

    # recording validation metrics
    if epoch%1 == 0:
        # notice! : this is mse loss
        net.eval()
        l = 0
        l_max = -float("inf")
        with torch.no_grad():
            for X, y in validate_iter:
                y_hat = net(X)
                l += loss(y_hat, y) * X.shape[0] # can be numerically unstable
                l_max = max(torch.max(torch.abs(y_hat - y)), l_max)
        l /= labels_validate.shape[0]
        loss_his[epoch] = l
        print(f'epoch {epoch + 1}, validation mse loss {l}')
        mse_l = l.item()
        wandb.log({"validate_mse_loss": mse_l},step=epoch)
        
        max_l = l_max.item()
        print(f'epoch {epoch + 1}, validation max loss {max_l}')
        wandb.log({"validate_max_loss": max_l},step=epoch)
        net.train()

    # recording test metrics
    if epoch%100 == 0:
        # notice! : this is mse loss
        net.eval()
        l = 0
        l_max = -float("inf")
        with torch.no_grad():
            for X, y in Test_iter:
                y_hat = net(X)
                l += loss(y_hat, y) * X.shape[0] # can be numerically unstable
                l_max = max(torch.max(torch.abs(y_hat - y)), l_max)
        l /= labels_Test.shape[0]
        loss_his[epoch] = l
        print(f'epoch {epoch + 1}, Test mse loss {l}')
        mse_l = l.item()
        wandb.log({"Test_mse_loss": mse_l},step=epoch)
        
        max_l = l_max.item()
        print(f'epoch {epoch + 1}, Test max loss {max_l}')
        wandb.log({"Test_max_loss": max_l},step=epoch)
        net.train()

    if False and epoch%10 == 0:
        example = torch.rand(1,6).to("cpu")
        traced_script_module = torch.jit.trace(net.to("cpu"), example)
        traced_script_module = traced_script_module.to("cpu")
        traced_script_module.save("./"+currentTime+"/traced_slinky_resnet_current.pt")
        traced_script_module.save("./traced_slinky_resnet_current.pt")
        shutil.copy("./traced_slinky_resnet_current.pt", os.path.join(wandb.run.dir, "traced_slinky_resnet_current.pt"))
        wandb.save("traced_slinky_resnet_current.pt")
        net.to(device)
        #print("batchnorm1d.running_mean: ", net.layer2[0].running_mean)
    
    
# saving the neural network parameters into matlab .mat file
net.eval()

# names = ['W1','b1','W2','b2','W3','b3','W4','b4','W5','b5','W6','b6','W7','b7','W8','b8','W9','b9','W10','b10','W11','b11','W12','b12','W13','b13','W14','b14','W15','b15','W16','b16','W17','b17','W18','b18','W19','b19','W20','b20','W21','b21','W22','b22','W23','b23','W24','b24','W25','b25','W26','b26','W27','b27','W28','b28','W29','b29','W30','b30']
# values = []
# # print(net[0].parameters().numpy()) 
# for params in net.parameters():
#   # print(type(params.detach().numpy()))
#   # print(params)
#   values.append(params.cpu().detach().numpy())

# # constructing the dict
# mdic = dict(zip(names,values))
# # using scipy.io
# scio.savemat("matlab_matrix_Resnet.mat", mdic)

# data recording using ONNX
# batch_size = 1
# x = torch.randn(batch_size,6).to(device)
# torch_out = net(x)

# torch.onnx.export(net,
#                   x,
#                   'slinky_Resnet.onnx',
#                   export_params=True,
#                   opset_version=10,
#                   do_constant_folding=True,
#                   input_names=['input'],
#                   output_names=['output'],
#                   dynamic_axes={'input':{0:'batch_size'},
#                                 'output':{0:'batch_size'}})

# recording the data in the BatchNorm layer
#batch_mean = net.layer2[0].running_mean
#batch_var = net.layer2[0].running_var
#names = ['batch_mean','batch_var']
#values = []
#values.append(batch_mean.cpu().detach().numpy())
#values.append(batch_var.cpu().detach().numpy())

# constructing the dict
mdic = dict(zip(names,values))
# using scipy.io
scio.savemat("batch_Resnet.mat", mdic)
print(net(torch.from_numpy(np.array([0.1, 0.1, 0, 0.001, 0.001, 0.001])).float().reshape(1,6).to(device)))

# constructing the libtorch model
#example = torch.rand(1,3).to(device)
#traced_script_module = torch.jit.trace(net, example).to("cpu")
#traced_script_module.save("traced_slinky_resnet.pt")

"""# Visualize residuals"""
#net([0.1,0.1,0])
# predicted_energy = net(features_train)

# plt.plot(labels_train.cpu().detach().numpy(), predicted_energy.cpu().detach().numpy())

# fig, ax = plt.subplots()
# color = 'tab:red'
# ax.plot(np.divide(labels_train.cpu().detach().numpy() - predicted_energy.cpu().detach().numpy(), labels_train.cpu().detach().numpy()),color=color)
# ax.set_xlabel('sample num',fontsize=12)
# ax.set_ylabel('relative error',fontsize=12)
# ax.set_title('error plot',fontsize=12)
# ax.tick_params(axis='y',labelcolor=color)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)

# ax2 = ax.twinx()
# color = 'tab:blue'
# ax2.set_ylabel('absolute error',color=color,fontsize=12)
# ax2.plot(labels_train.cpu().detach().numpy() - predicted_energy.cpu().detach().numpy(), color=color)
# ax2.tick_params(axis='y',labelcolor=color)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)
# fig.tight_layout()

# fig, ax = plt.subplots()
# ax.plot(np.arange(num_epochs),np.log10(loss_his),label='training loss')
# ax.set_xlabel('epoch',fontsize=12)
# ax.set_ylabel('log10(training loss)',fontsize=12)
# ax.set_title('training loss history',fontsize=12)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)

# fig, ax = plt.subplots()
# ax.plot(labels_train.cpu().detach().numpy())
# ax.set_xlabel('sample num',fontsize=12)
# ax.set_ylabel('label value',fontsize=12)
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)

# torch.mean((labels_train-predicted_energy)**2)

# fig = plt.figure()
# ax = fig.add_subplot(projection='3d')

# ax.set_xlabel('l_{i-1}')
# ax.set_ylabel('l_{i}')
# ax.set_zlabel('theta_i')

# xs = features_train[:,0].cpu().detach().numpy()
# ys = features_train[:,1].cpu().detach().numpy()
# zs = features_train[:,2].cpu().detach().numpy()
# v = labels_train.cpu().detach().numpy()
# c = np.abs(v)

# cmhot = plt.get_cmap("hot")
# ax.scatter(xs, ys, zs, v, s=5, c=c, cmap=cmhot)
# plt.show()


# fig = plt.figure()
# ax = fig.add_subplot(projection='3d')

# ax.set_xlabel('l_{i-1}')
# ax.set_ylabel('l_{i}')
# ax.set_zlabel('Energy')

# xs = features_train[:,0].cpu().detach().numpy()
# ys = features_train[:,1].cpu().detach().numpy()
# zs = labels_train[:,0].cpu().detach().numpy()

# ax.scatter(xs, ys, zs, 'tab:blue')

# ax.set_xlabel('l_{i-1}')
# ax.set_ylabel('l_{i}')
# ax.set_zlabel('Energy')

# xs = features_train[:,0].cpu().detach().numpy()
# ys = features_train[:,1].cpu().detach().numpy()
# zs = predicted_energy[:,0].cpu().detach().numpy()

# ax.scatter(xs, ys, zs, 'tab:red')
# plt.show()

example = torch.rand(1,6).to("cpu")
traced_script_module = torch.jit.trace(net.to("cpu"), example)
traced_script_module = traced_script_module.to("cpu")
traced_script_module.save("./"+currentTime+"/traced_slinky_resnet_final.pt")
traced_script_module.save("./traced_slinky_resnet_final.pt")
shutil.copy("./traced_slinky_resnet_final.pt", os.path.join(wandb.run.dir, "traced_slinky_resnet_final.pt"))
wandb.save("traced_slinky_resnet_final.pt")
net.to(device)

example = torch.rand(1,6).to("cpu")
traced_script_module = torch.jit.trace(best_model.to("cpu"), example)
traced_script_module = traced_script_module.to("cpu")
traced_script_module.save("./"+currentTime+"/traced_slinky_resnet_best.pt")
traced_script_module.save("./traced_slinky_resnet_best.pt")
shutil.copy("./traced_slinky_resnet_best.pt", os.path.join(wandb.run.dir, "traced_slinky_resnet_best.pt"))
wandb.save("traced_slinky_resnet_best.pt")
best_model.to(device)

wandb.finish()